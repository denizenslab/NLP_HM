{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLYMA8AZF0Dn"
      },
      "source": [
        "# Day 1 Tutorials: Natural Language Processing in Humans and Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORfy-AoqOnZY"
      },
      "source": [
        "## 1) Preprocessing text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0pZ1wnWO1Xk"
      },
      "source": [
        "In the lecture, we learned about preprocessing text. In this part of the tutorials, we will apply these concepts with popular python NLP libraries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQIShHKXG440"
      },
      "source": [
        "###1.1) Built-in python functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXsRX1nuHVSw"
      },
      "source": [
        "Python has some built-in functions that can help us preprocess the text. For example, we can make any string of text lower-case:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMxuqmjgGN1R"
      },
      "outputs": [],
      "source": [
        "raw_text = 'Once upon a time, in a Natural Language Processing Seminar at TU Berlin.'\n",
        "print(raw_text.lower())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJPeBfDlKrHO"
      },
      "source": [
        "###1.2) Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFgGQ6D-H-p5"
      },
      "source": [
        "We can also split a string of text at every space, a kind of very basic tokenizing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfHedU2qHy6a"
      },
      "outputs": [],
      "source": [
        "print(raw_text.lower().split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t2i1DZoIohf"
      },
      "source": [
        "Note that this function can also split at any given character by giving it as argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wapNLTaaIh2F"
      },
      "outputs": [],
      "source": [
        "print('Once-upon-a-time,-in-a-Natural-Language-Processing-Seminar-at-TU-Berlin.'.lower().split('-'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy6Ph77pLKqh"
      },
      "source": [
        "We will use several functions from the NLTK library. Python has many libraries that can help you do lots of stuff. You don't have to reinvent the wheel! When you use a library, it's always a good idea to read its documentation to learn how to use it and what you can do with it. You can read more about NLTK [here](https://www.nltk.org/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ8DsU7GMB4v"
      },
      "source": [
        "Let's import NLTK and instantiate a tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpneAR_4MBN2"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "tokenizer = nltk.tokenize.wordpunct_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE0lqnJyMQRp"
      },
      "source": [
        "We can now use this tokenizer on our previous text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plei2YngMAU9"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer(raw_text.lower())\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e_YlGyqMdWh"
      },
      "source": [
        "It did a better job than the built-in `split()` function, as it also separated words from the punctuation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYJFdaQG99ou"
      },
      "source": [
        "### 1.3) Stemminng and Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG007PfFMsYp"
      },
      "source": [
        "The NLTK library also provides functions that are very helpful for preprocessing of text in general, such as stemmers and lemmatizers. Let's have a look:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epjEx8pgLBZf"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "stemmer = nltk.stem.SnowballStemmer('english')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "print(stemmer.stem('walks'))\n",
        "print(lemmatizer.lemmatize('walks'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdZTUSgxNFFi"
      },
      "source": [
        "Stemming cuts the end of the word to its root (its stem) whereas lemmatizing uses more morphological information such as POS, plural, conjugation..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQVdttBdNIws"
      },
      "outputs": [],
      "source": [
        "print('Stem:', stemmer.stem('cries'))\n",
        "print('Lemma:', lemmatizer.lemmatize('cries'))\n",
        "\n",
        "print('Stem:', stemmer.stem('mice'))\n",
        "print('Lemma:', lemmatizer.lemmatize('mice'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvVT6q_HPZF9"
      },
      "source": [
        "###1.4) Break-out session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V79iGju1QAoV"
      },
      "source": [
        "These examples were all in English ([like most research in neuroscience](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-66132200236-4)). Try out some of these functions in other languages, maybe german?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHI1IwsNPqdx"
      },
      "outputs": [],
      "source": [
        "der_stemmer = nltk.stem.SnowballStemmer('german')\n",
        "...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu9ucexv-R_Z"
      },
      "source": [
        "## 2) Information Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlxxLDuz-h3i"
      },
      "source": [
        "### 2.1) POS tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ghk-q5P_Xkr"
      },
      "source": [
        "Next, we apply part-of-speech tagging to the tokenized words.\n",
        "\n",
        "Assigning each word a part-of-speech tag (e.g., noun, verb, adjective) helps in understanding the syntactic structure of the sentence.\n",
        "\n",
        "We will again use the NLTK library. The output will be a list of tuples where the first element is the word and the\n",
        "second element is the POS tag.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06IU5ASL_Zx-"
      },
      "outputs": [],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "pos_tags = nltk.tag.pos_tag(tokens)\n",
        "\n",
        "print(\"POS Tags:\", pos_tags)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwmbaOHCAdN0"
      },
      "source": [
        "### 2.2) Named Entity Recognition (NER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k209WsP-A0cO"
      },
      "source": [
        "We will use the [spacy](https://spacy.io/) library for NER."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEip1j63BQVG"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the text using the nlp model\n",
        "doc = nlp('Apple is looking at buying U.K. startup for $1 billion.')\n",
        "\n",
        "# Extract and print named entities\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib8BEwPNJfM2"
      },
      "source": [
        "### 2.3) Break-out session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdbqW_1tJ2Fc"
      },
      "source": [
        "Let's perform POS tagging and NER for our german sentence using Spacy.\n",
        "\n",
        "NOTE: we will now need a German language model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Gvq0qKYJ1dR"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"de_core_news_sm\")\n",
        "text = \"Angela Merkel besuchte Berlin und sprach mit Vertretern von Siemens.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41P3l7QRVfiF"
      },
      "source": [
        "## 3) Word vectors: GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc_ZQh2LYjK7"
      },
      "source": [
        "In the lecture, we studied word vectors. In this part of the tutorials, we will explore some of their capabilities. This was partly inspired by material from the [Natural Language Processing with Deep Learning class](http://web.stanford.edu/class/cs224n/index.html#schedule) from Stanford University.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56oCwf8lZED2"
      },
      "source": [
        "###3.1) Getting started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Dta4fPZ2Mi"
      },
      "source": [
        "For looking at word vectors, we will use Gensim. Gensim is a package for word and text similarity modeling. It is efficient and scalable, and quite widely used. You can read more about Gensim [here](https://radimrehurek.com/gensim/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hLQXylYWRME"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nujh1vovaD50"
      },
      "source": [
        "We will use the [GloVe word vectors](https://nlp.stanford.edu/projects/glove/) from Stanford. Gensim does not give them first class support, but allows you to convert a file of GloVe vectors into word2vec format.\n",
        "\n",
        "We will first download the embeddings in a zipped file.\n",
        "The `!` command in iPython allows us to call bash commands, just like you would do from the terminal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwMQRPxqaFUl"
      },
      "outputs": [],
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3YfVAJmaLHJ"
      },
      "outputs": [],
      "source": [
        "# Create a folder for the embeddings and unzip them there\n",
        "!mkdir word_vectors\n",
        "!unzip glove.6B.zip -d word_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wTJDiylaXEl"
      },
      "source": [
        "f-strings are a very convenient way to format strings, for example for filenames. Here, we can set `dim` to the dimension we want to use.\n",
        "\n",
        "You can play around with the dimensions as a mix between speed and smallness vs. quality. If you try out the 50d vectors, they basically work for similarity but clearly aren't as good for analogy problems. If you load the 300d vectors, they're even better than the 100d vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPR3goxJaSWw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "dim = 100\n",
        "word_dir = \"word_vectors\"\n",
        "glove_file = os.path.join(word_dir, f\"glove.6B.{dim}d.txt\")\n",
        "\n",
        "# this temporary file is used to convert the embeddings into the right format\n",
        "word2vec_glove_file = get_tmpfile(f\"glove.6B.{dim}d.word2vec.txt\")\n",
        "glove2word2vec(glove_file, word2vec_glove_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNDrbKK1ahCa"
      },
      "outputs": [],
      "source": [
        "# actually load the model\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UlRpg1TbgFj"
      },
      "source": [
        "###3.2) First look at the embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ6zG8PlbmfM"
      },
      "source": [
        "`model` now contains the word vectors in a lookup table by token!\n",
        "\n",
        "We can quickly check which words are in it, and what the values look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om3HvVbAbd0z"
      },
      "outputs": [],
      "source": [
        "model.get_vector('natural')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssLolAQnbrAD"
      },
      "outputs": [],
      "source": [
        "model['language']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clJ-iavUbskm"
      },
      "outputs": [],
      "source": [
        "model.get_vector('processing')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fzgsPgcbyZ_"
      },
      "source": [
        "These values are not very interesting per se. Each dimension has no explicit meaning. However we can see that some words are not in the vocabulary of the embeddings. They are called out-of-vocabulary (OOV) words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pe3Z6WrrbuTA"
      },
      "outputs": [],
      "source": [
        "model.get_vector('natuerlich')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wolPclj3cB23"
      },
      "outputs": [],
      "source": [
        "model.get_vector('lenguage')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Lmti9X6cEKx"
      },
      "outputs": [],
      "source": [
        "model.get_vector('Processing')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1xU4MxycL-K"
      },
      "outputs": [],
      "source": [
        "model.get_vector('problem,')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBo-UmxDcOUZ"
      },
      "outputs": [],
      "source": [
        "model.get_vector('tintinnabulation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j7pP2gmcUcx"
      },
      "source": [
        "Given these examples, can you give reasons why some words might be OOV?\n",
        "\n",
        "Can you think of other cases?\n",
        "\n",
        "What could be done to remedy these issues?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7KyF-ppcaSa"
      },
      "source": [
        "###3.3) Useful gensim functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WGh86_5eZ0m"
      },
      "source": [
        "The Gensim library comes with useful functions. Again, you don't have to reinvent the wheel!\n",
        "\n",
        "Let's look at the output of the `most_similar()` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RldfIfL6cZLj"
      },
      "outputs": [],
      "source": [
        "model.most_similar('obama')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OFNVj0OcQPb"
      },
      "outputs": [],
      "source": [
        "model.most_similar('merkel')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57Eqdk4UegXP"
      },
      "outputs": [],
      "source": [
        "model.most_similar('berlin')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zPYPm1MemNU"
      },
      "source": [
        "Do you think these results make sense? What is the number next to the word?\n",
        "\n",
        "The default parameter is `positive=`, but the `most_similar()` function can also be used by inputting words as `negative=`. Words will then contribute with the negative of their vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCsUiotOeibf"
      },
      "outputs": [],
      "source": [
        "model.most_similar(positive='banana')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfhvgM9zepfv"
      },
      "outputs": [],
      "source": [
        "model.most_similar(negative='banana')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKYEk6lNevGM"
      },
      "source": [
        "As you can see, the opposite of a given word is not very relevant, why do you think that is the case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwsOnONpexgK"
      },
      "source": [
        "###3.4) Creating a function to do analogies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuvE5LVAe3xE"
      },
      "source": [
        "The geometry of word vectors allows us to use them for many word relations, such as analogies: A is to B what X is to Y.\n",
        "\n",
        "For example, 'Berlin' is to 'Germany' what 'Rome' is to 'Italy'.\n",
        "\n",
        "Write a function which does this:\n",
        "\n",
        "```\n",
        "analogy('berlin', 'germany', 'rome')\n",
        "> 'italy'\n",
        "```\n",
        "\n",
        "You can draw the geometry of these vectors on a piece of paper to help you.\n",
        "(Hint: all you need is the function `most_similar`, with the `positive=` and `negative=` parameters).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x7hzJJWereE"
      },
      "outputs": [],
      "source": [
        "def analogy(A, B, X):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjgZnFRSe9Ja"
      },
      "outputs": [],
      "source": [
        "analogy('berlin', 'germany', 'rome')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1M5uG5WfElT"
      },
      "source": [
        "Let's check that the function works on other types of analogies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYCnnt4IfAqT"
      },
      "outputs": [],
      "source": [
        "analogy('germany', 'beer', 'france')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCYHsNdMfG6_"
      },
      "outputs": [],
      "source": [
        "analogy('japan', 'japanese', 'australia')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7jvaF05fZE9"
      },
      "outputs": [],
      "source": [
        "analogy('obama', 'clinton', 'reagan')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBe-WD9gfhOI"
      },
      "outputs": [],
      "source": [
        "analogy('tall', 'tallest', 'long')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8ok6_tKfmQC"
      },
      "outputs": [],
      "source": [
        "analogy('walk', 'walked', 'see')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMbcER0efpOb"
      },
      "outputs": [],
      "source": [
        "analogy('good', 'fantastic', 'bad')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWQdABQxfxGf"
      },
      "source": [
        "###3.5) Other functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvQ_hhRCgHgs"
      },
      "source": [
        "[Here is the documentation](https://radimrehurek.com/gensim/models/keyedvectors.html) of the `KeyedVectors` model in `Gensim`.\n",
        "You can even check the [source code](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py#L776) of given functions, for example `most_similar()`.\n",
        "\n",
        "Let's look at other functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqOaNYE5fter"
      },
      "outputs": [],
      "source": [
        "model.doesnt_match([\"breakfast\", \"cereal\", \"dinner\", \"lunch\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM382Fa3gTAs"
      },
      "source": [
        "Try any other function that you find interesting!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb5GqFTVgMzD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqJ3RjoCgT-8"
      },
      "source": [
        "###3.6) Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIaH8avNgkJy"
      },
      "source": [
        "Let's look at the values of a few word vectors. We will use the\n",
        "very popular library [Matplotlib](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7piejeg8ghlZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tools for plotting\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# This creates a figure, you can define the size\n",
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "word1 = 'banana'\n",
        "word2 = 'apple'\n",
        "word3 = 'car'\n",
        "plt.plot(model.get_vector(word1), label=word1)\n",
        "plt.plot(model.get_vector(word2), label=word2)\n",
        "plt.plot(model.get_vector(word3), label=word3)\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOygdNlphD9a"
      },
      "source": [
        "It is always good to add labels on the x and y axes of your plots, as well as a title. Can you find a way to do that?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxihcUkbhH5L"
      },
      "source": [
        "As we said, the absolute values of the vectors are not very interpretable. We are more interested in the relative values between words. However, in such a high dimensional space, they are tough to visualize.\n",
        "\n",
        "This is why dimensionality reduction techniques, such as PCA or t-SNE, are often used to visualize word vectors. Let's use PCA to show some word vectors as a cloud of points. We will use the PCA implementation for the [scikit-learn library](hthttps://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.htmltps://)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGGyvVV1hJSa"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "def display_pca_scatterplot(model, words):\n",
        "\n",
        "    # gets the vectors corresponing to the words in the model\n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "\n",
        "    # fit the PCA is keep the first 2 dimensions\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "\n",
        "    # create a scatter plot\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "\n",
        "    # add the words close to the points\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCCPkkowhMwG"
      },
      "outputs": [],
      "source": [
        "display_pca_scatterplot(model,\n",
        "                        ['coffee', 'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', 'water',\n",
        "                         'spaghetti', 'borscht', 'hamburger', 'pizza', 'falafel', 'sushi', 'meatballs',\n",
        "                         'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard',\n",
        "                         'frog', 'toad', 'monkey', 'ape', 'kangaroo', 'wombat', 'wolf',\n",
        "                         'france', 'paris', 'germany', 'berlin', 'italy', 'rome', 'hungary', 'luxembourg', 'australia', 'fiji', 'china',\n",
        "                         'homework', 'assignment', 'problem', 'exam', 'test', 'class',\n",
        "                         'school', 'college', 'university', 'institute'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GVf9BAZhU27"
      },
      "source": [
        "Do the 2D visualizations make sense with the analogies we computed earlier?\n",
        "\n",
        "Go ahead an plot any other word wou would like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5MhDC0fhVS7"
      },
      "outputs": [],
      "source": [
        "display_pca_scatterplot(model,\n",
        "                        [''])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
