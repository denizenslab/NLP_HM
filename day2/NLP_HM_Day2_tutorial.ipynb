{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Day 2 Tutorials: Natural Language Processing in Humans and Machines\n",
        "\n",
        "## NLP Advanced: Sequential Models\n",
        "In this notebook, we'll explore different NLP models from traditional RNNs to the latest language models like GPT-2 and BERT.\n",
        "\n",
        "We'll cover:\n",
        "- Recurrent Neural Networks (RNN)\n",
        "- Long Short-Term Memory (LSTM)\n",
        "- Generative Pre-trained Transformers (GPT-2)\n",
        "- Bidirectional Encoder Representations from Transformers (BERT)\n",
        "\n",
        "Let's get started!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mnSJWI66vjwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy input sentence\n",
        "sentence = \"The curious AI explored the world of words, crafting stories that amazed even the most skeptical humans.\""
      ],
      "metadata": {
        "id": "QbpfRtFyyXJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Recurrent Neural Networks (RNN)\n",
        "\n",
        " RNNs are a type of neural network designed for sequence data. They maintain a hidden state that captures the sequence's context, making them suitable for tasks like language modeling and time-series prediction. However, RNNs struggle with long-term dependencies due to the vanishing gradient problem"
      ],
      "metadata": {
        "id": "CqdDfBIaxATJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple RNN\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), hidden_size)  # Initialize hidden state\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])  # Take the last hidden state\n",
        "        return out\n",
        "\n",
        "\n",
        "input_size = len(sentence.split())  # Simulate the word embeddings size\n",
        "hidden_size = 20\n",
        "output_size = 5\n",
        "rnn_model = SimpleRNN(input_size, hidden_size, output_size)\n",
        "\n",
        "dummy_input = torch.randn(3, len(sentence.split()), input_size)  # Random input to simulate the sentence\n",
        "output = rnn_model(dummy_input)\n",
        "print(\"RNN Output Embeddings for the sentence:\", output)\n",
        "print(\"shape:\", output.shape)\n"
      ],
      "metadata": {
        "id": "O31lOz_gxVQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Long Short-Term Memory (LSTM)\n",
        "\n",
        "LSTM networks address the limitations of RNNs by introducing gates (forget, input, and output) to control the flow of information. This allows LSTMs to capture long-term dependencies more effectively, making them suitable for tasks like machine translation and text generation.\n",
        "\n"
      ],
      "metadata": {
        "id": "f7Zm8H8hxls-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), hidden_size)\n",
        "        c0 = torch.zeros(1, x.size(0), hidden_size)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Example usage\n",
        "input_size = len(sentence.split())  # Simulate the word embeddings size\n",
        "hidden_size = 20\n",
        "output_size = 5\n",
        "lstm_model = SimpleLSTM(input_size, hidden_size, output_size)\n",
        "\n",
        "dummy_input = torch.randn(3, len(sentence.split()), input_size)  # Random input to simulate the sentence\n",
        "output = lstm_model(dummy_input)\n",
        "print(\"LSTM Output Embeddings for the sentence:\", output)\n",
        "print(\"shape:\", output.shape)"
      ],
      "metadata": {
        "id": "fkOE_1bHxq2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Generative Pre-trained Transformer (GPT-2)\n",
        "\n",
        "GPT-2 is a transformer-based model designed for text generation tasks. It is trained to predict the next token in a sequence and can generate coherent text. Hugging Faceâ€™s transformers library provides easy access to GPT-2 for various NLP tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "LuPe3Bynx6Bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "\n",
        "# Load GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2Model.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Example input sentence\n",
        "input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Get hidden states from GPT-2\n",
        "outputs = model(input_ids)\n",
        "hidden_states = outputs.last_hidden_state\n",
        "\n",
        "print(\"GPT-2 Hidden States for the sentence:\", hidden_states)\n",
        "print(\"shape:\", hidden_states.shape)\n"
      ],
      "metadata": {
        "id": "wRoflnGlyIgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Bidirectional Encoder Representations from Transformers (BERT)\n",
        "\n",
        "BERT is a transformer-based model pre-trained on masked language modeling and next sentence prediction tasks. It generates deep contextualized embeddings that can be fine-tuned for a variety of downstream tasks like text classification, question answering, and more.\n",
        "\n"
      ],
      "metadata": {
        "id": "K33ktqAUyHPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Example input sentence\n",
        "input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Get BERT embeddings\n",
        "outputs = model(input_ids)\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "print(\"BERT Embeddings for the sentence:\", last_hidden_states)\n",
        "print(\"shape:\", last_hidden_states.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "iC37RgNLyOpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Breakout Session: Exploring Hugging Face and Generating Embeddings with Pre-trained Models!\n",
        "\n",
        "\n",
        "- Go explore [Hugging Face](https://huggingface.co/).\n",
        "- Choose an appropriate pre-trained model for generating sentence embeddings.\n",
        "- Use the model to generate embeddings for the given sentence.\n",
        "\n"
      ],
      "metadata": {
        "id": "UrhG0Kpc2Xi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Replace 'your-chosen-model' with the model you picked from Hugging Face\n",
        "model_name = \"your-chosen-model\"\n",
        "\n",
        "# Load tokenizer and model from Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"The curious AI explored the world of words, crafting stories that amazed even the most skeptical humans.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "eYD3dCs-3XhX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}