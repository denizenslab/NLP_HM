{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Day 4 Tutorials: Natural Language Processing in Humans and Machines\n",
        "\n",
        "## NLP in Humans: Insights from Neuroscience research"
      ],
      "metadata": {
        "id": "FcP5kS7M7CuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Encoding models"
      ],
      "metadata": {
        "id": "lmENziL19g0N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4trKmlg64R2L"
      },
      "source": [
        "## Change runtime to use a GPU\n",
        "\n",
        "This tutorial is much faster when a GPU is available to run the computations.\n",
        "In Google Colab you can request access to a GPU by changing the runtime type.\n",
        "To do so, click the following menu options in Google Colab:\n",
        "\n",
        "(Menu) \"Runtime\" -> \"Change runtime type\" -> \"Hardware accelerator\" -> \"GPU\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDaiVxKK4R2M"
      },
      "source": [
        "### Setup Google Colab, download the data and install all required dependencies\n",
        "\n",
        "Uncomment and run the following cell to download the required packages.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5KoEOMb4R2M"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"you@example.com\" && git config --global user.name \"Your Name\"\n",
        "!wget -O- http://neuro.debian.net/lists/jammy.us-ca.libre | sudo tee /etc/apt/sources.list.d/neurodebian.sources.list\n",
        "!apt-key adv --recv-keys --keyserver hkps://keyserver.ubuntu.com 0xA5D32F012649A5A9 > /dev/null\n",
        "!apt-get -qq update > /dev/null\n",
        "!apt-get install -qq inkscape git-annex-standalone > /dev/null\n",
        "!pip install -q voxelwise_tutorials\n",
        "\n",
        "# this is what each command does:\n",
        "# - Set up an email and username to use git, git-annex, and datalad (required to download the data)\n",
        "# - Add NeuroDebian to the package sources\n",
        "# - Update the gpg keys to use NeuroDebian\n",
        "# - Update the list of available packages\n",
        "# - Install Inkscape to use more features from Pycortex, and install git-annex to download the data\n",
        "# - Install the tutorial helper package, and all the required dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsPKmseB4R2P"
      },
      "source": [
        "Now run the following cell to download the data for the tutorials.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3ug_TE74R2P"
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.io import download_datalad\n",
        "\n",
        "DATAFILES = [\n",
        "    \"features/motion_energy.hdf\",\n",
        "    \"features/wordnet.hdf\",\n",
        "    \"mappers/S01_mappers.hdf\",\n",
        "    \"responses/S01_responses.hdf\",\n",
        "]\n",
        "\n",
        "source = \"https://gin.g-node.org/gallantlab/shortclips\"\n",
        "destination = \"/content/shortclips\"\n",
        "\n",
        "for datafile in DATAFILES:\n",
        "    local_filename = download_datalad(\n",
        "        datafile,\n",
        "        destination=destination,\n",
        "        source=source\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9cQW37m4R2Q"
      },
      "source": [
        "Now run the following cell to set up the environment variables for the\n",
        "tutorials and pycortex.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mMCgYxc4R2Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['VOXELWISE_TUTORIALS_DATA'] = \"/content\"\n",
        "\n",
        "import sklearn\n",
        "sklearn.set_config(assume_finite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJPqJq3k4R2Q"
      },
      "source": [
        "Your Google Colab environment is now set up for the voxelwise tutorials.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aWNPCkg4R2Q"
      },
      "outputs": [],
      "source": [
        "%reset -f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JqacM4A4R2R"
      },
      "source": [
        "\n",
        "# Compute the explainable variance\n",
        "\n",
        "Before fitting any voxelwise model to fMRI responses, it is good practice to\n",
        "quantify the amount of signal in the test set that can be predicted by an\n",
        "encoding model. This quantity is called the *explainable variance*.\n",
        "\n",
        "The measured signal can be decomposed into a sum of two components: the\n",
        "stimulus-dependent signal and noise. If we present the same stimulus multiple\n",
        "times and we record brain activity for each repetition, the stimulus-dependent\n",
        "signal will be the same across repetitions while the noise will vary across\n",
        "repetitions. In voxelwise modeling, the features used to model brain activity\n",
        "are the same for each repetition of the stimulus. Thus, encoding models will\n",
        "predict only the repeatable stimulus-dependent signal.\n",
        "\n",
        "The stimulus-dependent signal can be estimated by taking the mean of brain\n",
        "responses over repeats of the same stimulus or experiment. The variance of the\n",
        "estimated stimulus-dependent signal, which we call the explainable variance, is\n",
        "proportional to the maximum prediction accuracy that can be obtained by a\n",
        "voxelwise encoding model in the test set.\n",
        "\n",
        "Mathematically, let $y_i, i = 1 \\dots N$ be the measured signal in a\n",
        "voxel for each of the $N$ repetitions of the same stimulus and\n",
        "$\\bar{y} = \\frac{1}{N}\\sum_{i=1}^Ny_i$ the average brain response\n",
        "across repetitions. For each repeat, we define the residual timeseries between\n",
        "brain response and average brain response as $r_i = y_i - \\bar{y}$. The\n",
        "explainable variance (EV) is estimated as\n",
        "\n",
        "\\begin{align}\\text{EV} = \\frac{1}{N}\\sum_{i=1}^N\\text{Var}(y_i) - \\frac{N}{N-1}\\sum_{i=1}^N\\text{Var}(r_i)\\end{align}\n",
        "\n",
        "\n",
        "In the literature, the explainable variance is also known as the *signal\n",
        "power*. For more information, see these references [1]_ [2]_ [3]_.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwYIiNB74R2R"
      },
      "source": [
        "## Path of the data directory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rKXiUP74R2R"
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.io import get_data_home\n",
        "\n",
        "directory = get_data_home(dataset=\"shortclips\")\n",
        "print(directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPm5-DH34R2R"
      },
      "outputs": [],
      "source": [
        "# modify to use another subject\n",
        "subject = \"S01\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNZMK8rh4R2S"
      },
      "source": [
        "## Compute the explainable variance\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga1vcikv4R2S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from voxelwise_tutorials.io import load_hdf5_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcfkQ4Y-4R2S"
      },
      "source": [
        "First, we load the fMRI responses on the test set, which contains brain\n",
        "responses to ten (10) repeats of the same stimulus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5R0PDq8k4R2S"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "file_name = os.path.join(directory, 'responses', f'{subject}_responses.hdf')\n",
        "Y_test = load_hdf5_array(file_name, key=\"Y_test\")\n",
        "print(\"(n_repeats, n_samples_test, n_voxels) =\", Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkLXdQIZ4R2S"
      },
      "source": [
        "Then, we compute the explainable variance for each voxel.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEzq-5DK4R2S"
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.utils import explainable_variance\n",
        "\n",
        "ev = explainable_variance(Y_test)\n",
        "print(\"(n_voxels,) =\", ev.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIQqgeGK4R2T"
      },
      "source": [
        "## Map to subject flatmap\n",
        "\n",
        "To better understand the distribution of explainable variance, we map the\n",
        "values to the subject brain. This can be done with [pycortex](https://gallantlab.github.io/pycortex/), which can create interactive 3D\n",
        "viewers to be displayed in any modern browser. ``pycortex`` can also display\n",
        "flattened maps of the cortical surface to visualize the entire cortical\n",
        "surface at once.\n",
        "\n",
        "Here, we do not share the anatomical information of the subjects for privacy\n",
        "concerns. Instead, we provide two mappers:\n",
        "\n",
        "- to map the voxels to a (subject-specific) flatmap\n",
        "- to map the voxels to the Freesurfer average cortical surface (\"fsaverage\")\n",
        "\n",
        "The first mapper is 2D matrix of shape (n_pixels, n_voxels) that maps each\n",
        "voxel to a set of pixel in a flatmap. The matrix is efficiently stored in a\n",
        "``scipy`` sparse CSR matrix. The function ``plot_flatmap_from_mapper``\n",
        "provides an example of how to use the mapper and visualize the flatmap.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXWO2t6E4R2U"
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.viz import plot_flatmap_from_mapper\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mapper_file = os.path.join(directory, 'mappers', f'{subject}_mappers.hdf')\n",
        "plot_flatmap_from_mapper(ev, mapper_file, vmin=0, vmax=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YEQU7394R2U"
      },
      "source": [
        "This figure is a flattened map of the cortical surface. A number of regions\n",
        "of interest (ROIs) have been labeled to ease interpretation. If you have\n",
        "never seen such a flatmap, we recommend taking a look at a [pycortex brain\n",
        "viewer](https://www.gallantlab.org/brainviewer/Deniz2019), which displays\n",
        "the brain in 3D. In this viewer, press \"I\" to inflate the brain, \"F\" to\n",
        "flatten the surface, and \"R\" to reset the view (or use the ``surface/unfold``\n",
        "cursor on the right menu). Press \"H\" for a list of all keyboard shortcuts.\n",
        "This viewer should help you understand the correspondence between the flatten\n",
        "and the folded cortical surface of the brain.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziXNsFJy4R2U"
      },
      "source": [
        "On this flatmap, we can see that the explainable variance is mainly located\n",
        "in the visual cortex, in early visual regions like V1, V2, V3, or in\n",
        "higher-level regions like EBA, FFA or IPS. This is expected since this\n",
        "dataset contains responses to a visual stimulus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_yPB3xC4R2U"
      },
      "source": [
        "## Map to \"fsaverage\"\n",
        "\n",
        "The second mapper we provide maps the voxel data to a Freesurfer\n",
        "average surface (\"fsaverage\"), that can be used in ``pycortex``.\n",
        "\n",
        "First, let's download the \"fsaverage\" surface.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "II0Awwwd4R2c"
      },
      "outputs": [],
      "source": [
        "import cortex\n",
        "\n",
        "surface = \"fsaverage\"\n",
        "\n",
        "if not hasattr(cortex.db, surface):\n",
        "    cortex.utils.download_subject(subject_id=surface,\n",
        "                                  pycortex_store=cortex.db.filestore)\n",
        "    cortex.db.reload_subjects()  # force filestore reload\n",
        "    assert hasattr(cortex.db, surface)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bUSZuSv4R2c"
      },
      "source": [
        "Then, we load the \"fsaverage\" mapper. The mapper is a matrix of shape\n",
        "(n_vertices, n_voxels), which maps each voxel to some vertices in the\n",
        "fsaverage surface. It is stored as a sparse CSR matrix. The mapper is applied\n",
        "with a dot product ``@`` (equivalent to ``np.dot``).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2ej045Z4R2d"
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.io import load_hdf5_sparse_array\n",
        "\n",
        "voxel_to_fsaverage = load_hdf5_sparse_array(mapper_file,\n",
        "                                            key='voxel_to_fsaverage')\n",
        "ev_projected = voxel_to_fsaverage @ ev\n",
        "print(\"(n_vertices,) =\", ev_projected.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTX95BIq4R2d"
      },
      "source": [
        "We can then create a ``Vertex`` object in ``pycortex``, containing the\n",
        "projected data. This object can be used either in a ``pycortex`` interactive\n",
        "3D viewer, or in a ``matplotlib`` figure showing only the flatmap.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW8px2X14R2d"
      },
      "outputs": [],
      "source": [
        "vertex = cortex.Vertex(ev_projected, surface, vmin=0, vmax=0.7, cmap='viridis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjNRIhmI4R2e"
      },
      "outputs": [],
      "source": [
        "from cortex.testing_utils import has_installed\n",
        "\n",
        "fig = cortex.quickshow(vertex, colorbar_location='right',\n",
        "                       with_rois=has_installed(\"inkscape\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COYEYOxU4R2e"
      },
      "source": [
        "## References\n",
        "\n",
        ".. [1] Sahani, M., & Linden, J. F. (2003). How linear are auditory cortical\n",
        "   responses?. Advances in neural information processing systems, 125-132.\n",
        "\n",
        ".. [2] Hsu, A., Borst, A., & Theunissen, F. E. (2004). Quantifying\n",
        "   variability in neural responses and its application for the validation of\n",
        "   model predictions. Network: Computation in Neural Systems, 15(2), 91-109.\n",
        "\n",
        ".. [3] Schoppe, O., Harper, N. S., Willmore, B. D., King, A. J., & Schnupp,\n",
        "       J. W. (2016). Measuring the performance of neural models. Frontiers in\n",
        "       computational neuroscience, 10, 10.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxrVZqP24R2e"
      },
      "outputs": [],
      "source": [
        "%reset -f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQyXJsmC4R2m"
      },
      "source": [
        "\n",
        "# Fit a ridge model with wordnet features\n",
        "\n",
        "In this example, we model the fMRI responses with semantic \"wordnet\" features,\n",
        "manually annotated on each frame of the movie stimulus. The model is a\n",
        "regularized linear regression model, known as ridge regression. Since this\n",
        "model is used to predict brain activity from the stimulus, it is called a\n",
        "(voxelwise) encoding model.\n",
        "\n",
        "This example reproduces part of the analysis described in Huth et al (2012)\n",
        "[1]_. See this publication for more details about the experiment, the wordnet\n",
        "features, along with more results and more discussions.\n",
        "\n",
        "*Wordnet features:* The features used in this example are semantic labels\n",
        "manually annotated on each frame of the movie stimulus. The semantic labels\n",
        "include nouns (such as \"woman\", \"car\", or \"building\") and verbs (such as\n",
        "\"talking\", \"touching\", or \"walking\"), for a total of 1705 distinct category\n",
        "labels. To interpret our model, labels can be organized in a graph of semantic\n",
        "relationship based on the [Wordnet](https://wordnet.princeton.edu/) dataset.\n",
        "\n",
        "*Summary:* We first concatenate the features with multiple temporal delays to\n",
        "account for the slow hemodynamic response. We then use linear regression to fit\n",
        "a predictive model of brain activity. The linear regression is regularized to\n",
        "improve robustness to correlated features and to improve generalization\n",
        "performance. The optimal regularization hyperparameter is selected over a\n",
        "grid-search with cross-validation. Finally, the model generalization\n",
        "performance is evaluated on a held-out test set, comparing the model\n",
        "predictions to the corresponding ground-truth fMRI responses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-jN3Uch4R2n"
      },
      "outputs": [],
      "source": [
        "# modify to use another subject\n",
        "subject = \"S01\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzX8CCU-4R2n"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "We first load the fMRI responses. These responses have been preprocessed as\n",
        "described in [1]_. The data is separated into a training set ``Y_train`` and a\n",
        "testing set ``Y_test``. The training set is used for fitting models, and\n",
        "selecting the best models and hyperparameters. The test set is later used\n",
        "to estimate the generalization performance of the selected model. The\n",
        "test set contains multiple repetitions of the same experiment to estimate\n",
        "an upper bound of the model prediction accuracy (cf. previous example).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwuVgr5j4R2n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from voxelwise_tutorials.io import load_hdf5_array\n",
        "\n",
        "file_name = os.path.join(directory, \"responses\", f\"{subject}_responses.hdf\")\n",
        "Y_train = load_hdf5_array(file_name, key=\"Y_train\")\n",
        "Y_test = load_hdf5_array(file_name, key=\"Y_test\")\n",
        "\n",
        "print(\"(n_samples_train, n_voxels) =\", Y_train.shape)\n",
        "print(\"(n_repeats, n_samples_test, n_voxels) =\", Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDQ_IpdJ4R2n"
      },
      "source": [
        "If we repeat an experiment multiple times, part of the fMRI responses might\n",
        "change. However the modeling features do not change over the repeats, so the\n",
        "voxelwise encoding model will predict the same signal for each repeat. To\n",
        "have an upper bound of the model prediction accuracy, we keep only the\n",
        "repeatable part of the signal by averaging the test repeats.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd0RQesY4R2n"
      },
      "outputs": [],
      "source": [
        "Y_test = Y_test.mean(0)\n",
        "\n",
        "print(\"(n_samples_test, n_voxels) =\", Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nduMpoCQ4R2n"
      },
      "source": [
        "We fill potential NaN (not-a-number) values with zeros.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umBpnmvA4R2n"
      },
      "outputs": [],
      "source": [
        "Y_train = np.nan_to_num(Y_train)\n",
        "Y_test = np.nan_to_num(Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd1IT8xm4R2o"
      },
      "source": [
        "Then, we load the semantic \"wordnet\" features, extracted from the stimulus at\n",
        "each time point. The features corresponding to the training set are noted\n",
        "``X_train``, and the features corresponding to the test set are noted\n",
        "``X_test``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfgcwWgE4R2o"
      },
      "outputs": [],
      "source": [
        "feature_space = \"wordnet\"\n",
        "\n",
        "file_name = os.path.join(directory, \"features\", f\"{feature_space}.hdf\")\n",
        "X_train = load_hdf5_array(file_name, key=\"X_train\")\n",
        "X_test = load_hdf5_array(file_name, key=\"X_test\")\n",
        "\n",
        "print(\"(n_samples_train, n_features) =\", X_train.shape)\n",
        "print(\"(n_samples_test, n_features) =\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVUbcjH74R2o"
      },
      "source": [
        "## Define the cross-validation scheme\n",
        "\n",
        "To select the best hyperparameter through cross-validation, we must define a\n",
        "cross-validation splitting scheme. Because fMRI time-series are\n",
        "autocorrelated in time, we should preserve as much as possible the temporal\n",
        "correlation. In other words, because consecutive time samples are correlated,\n",
        "we should not put one time sample in the training set and the immediately\n",
        "following time sample in the validation set. Thus, we define here a\n",
        "leave-one-run-out cross-validation split that keeps each recording run\n",
        "intact.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUJ4KLdM4R2o"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import check_cv\n",
        "from voxelwise_tutorials.utils import generate_leave_one_run_out\n",
        "\n",
        "# indice of first sample of each run\n",
        "run_onsets = load_hdf5_array(file_name, key=\"run_onsets\")\n",
        "print(run_onsets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3LEPi8t4R2o"
      },
      "source": [
        "We define a cross-validation splitter, compatible with ``scikit-learn`` API.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhf-MxRW4R2o"
      },
      "outputs": [],
      "source": [
        "n_samples_train = X_train.shape[0]\n",
        "cv = generate_leave_one_run_out(n_samples_train, run_onsets)\n",
        "cv = check_cv(cv)  # copy the cross-validation splitter into a reusable list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goIUUBS64R2o"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "Now, let's define the model pipeline.\n",
        "\n",
        "We first center the features, since we will not use an intercept. The mean\n",
        "value in fMRI recording is non-informative, so each run is detrended and\n",
        "demeaned independently, and we do not need to predict an intercept value in\n",
        "the linear model.\n",
        "\n",
        "However, we prefer to avoid normalizing by the standard deviation of each\n",
        "feature. If the features are extracted in a consistent way from the stimulus,\n",
        "their relative scale is meaningful. Normalizing them independently from each\n",
        "other would remove this information. Moreover, the wordnet features are\n",
        "one-hot-encoded, which means that each feature is either present (1) or not\n",
        "present (0) in each sample. Normalizing one-hot-encoded features is not\n",
        "recommended, since it would scale disproportionately the infrequent features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNFS9AaH4R2o"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler(with_mean=True, with_std=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpV_JY3F4R2p"
      },
      "source": [
        "Then we concatenate the features with multiple delays to account for the\n",
        "hemodynamic response. Due to neurovascular coupling, the recorded BOLD signal\n",
        "is delayed in time with respect to the stimulus onset. With different delayed\n",
        "versions of the features, the linear regression model will weigh each delayed\n",
        "feature with a different weight to maximize the predictions. With a sample\n",
        "every 2 seconds, we typically use 4 delays [1, 2, 3, 4] to cover the\n",
        "hemodynamic response peak. In the next example, we further describe this\n",
        "hemodynamic response estimation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izD6CorV4R2p"
      },
      "outputs": [],
      "source": [
        "from voxelwise_tutorials.delayer import Delayer\n",
        "delayer = Delayer(delays=[1, 2, 3, 4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbOOOoBy4R2p"
      },
      "source": [
        "Finally, we use a ridge regression model. Ridge regression is a linear\n",
        "regression with L2 regularization. The L2 regularization improves robustness\n",
        "to correlated features and improves generalization performance. The L2\n",
        "regularization is controlled by a hyperparameter ``alpha`` that needs to be\n",
        "tuned for each dataset. This regularization hyperparameter is usually\n",
        "selected over a grid search with cross-validation, selecting the\n",
        "hyperparameter that maximizes the predictive performances on the validation\n",
        "set. See the previous example for more details about ridge regression and\n",
        "hyperparameter selection.\n",
        "\n",
        "For computational reasons, when the number of features is larger than the\n",
        "number of samples, it is more efficient to solve ridge regression using the\n",
        "(equivalent) dual formulation [2]_. This dual formulation is equivalent to\n",
        "kernel ridge regression with a linear kernel. Here, we have 3600 training\n",
        "samples, and 1705 * 4 = 6820 features (we multiply by 4 since we use 4 time\n",
        "delays), therefore it is more efficient to use kernel ridge regression.\n",
        "\n",
        "With one target, we could directly use the pipeline in ``scikit-learn``'s\n",
        "``GridSearchCV``, to select the optimal regularization hyperparameter\n",
        "(``alpha``) over cross-validation. However, ``GridSearchCV`` can only\n",
        "optimize a single score across all voxels (targets). Thus, in the\n",
        "multiple-target case, ``GridSearchCV`` can only optimize (for example) the\n",
        "mean score over targets. Here, we want to find a different optimal\n",
        "hyperparameter per target/voxel, so we use the package [himalaya](https://github.com/gallantlab/himalaya) which implements a\n",
        "``scikit-learn`` compatible estimator ``KernelRidgeCV``, with hyperparameter\n",
        "selection independently on each target.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4eefVoT4R2p"
      },
      "outputs": [],
      "source": [
        "from himalaya.kernel_ridge import KernelRidgeCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcrPGWOQ4R2p"
      },
      "source": [
        "``himalaya`` implements different computational backends,\n",
        "including two backends that use GPU for faster computations. The two\n",
        "available GPU backends are \"torch_cuda\" and \"cupy\". (Each backend is only\n",
        "available if you installed the corresponding package with CUDA enabled. Check\n",
        "the ``pytorch``/``cupy`` documentation for install instructions.)\n",
        "\n",
        "Here we use the \"torch_cuda\" backend, but if the import fails we continue\n",
        "with the default \"numpy\" backend. The \"numpy\" backend is expected to be\n",
        "slower since it only uses the CPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vePW4H_J4R2p"
      },
      "outputs": [],
      "source": [
        "from himalaya.backend import set_backend\n",
        "backend = set_backend(\"torch_cuda\", on_error=\"warn\")\n",
        "print(backend)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZjbczdK4R2p"
      },
      "source": [
        "To speed up model fitting on GPU, we use single precision float numbers.\n",
        "(This step probably does not change significantly the performances on non-GPU\n",
        "backends.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU5XCFTw4R2p"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.astype(\"float32\")\n",
        "X_test = X_test.astype(\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH16m1H-4R2p"
      },
      "source": [
        "Since the scale of the regularization hyperparameter ``alpha`` is unknown, we\n",
        "use a large logarithmic range, and we will check after the fit that best\n",
        "hyperparameters are not all on one range edge.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfTd7XiN4R2q"
      },
      "outputs": [],
      "source": [
        "alphas = np.logspace(1, 20, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qefGo8mz4R2q"
      },
      "source": [
        "We also indicate some batch sizes to limit the GPU memory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juVA1qPj4R2q"
      },
      "outputs": [],
      "source": [
        "kernel_ridge_cv = KernelRidgeCV(\n",
        "    alphas=alphas, cv=cv,\n",
        "    solver_params=dict(n_targets_batch=500, n_alphas_batch=5,\n",
        "                       n_targets_batch_refit=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpPt-ejD4R2q"
      },
      "source": [
        "Finally, we use a ``scikit-learn`` ``Pipeline`` to link the different steps\n",
        "together. A ``Pipeline`` can be used as a regular estimator, calling\n",
        "``pipeline.fit``, ``pipeline.predict``, etc. Using a ``Pipeline`` can be\n",
        "useful to clarify the different steps, avoid cross-validation mistakes, or\n",
        "automatically cache intermediate results. See the ``scikit-learn``\n",
        "[documentation](https://scikit-learn.org/stable/modules/compose.html) for\n",
        "more information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qK-mqGzw4R2q"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "pipeline = make_pipeline(\n",
        "    scaler,\n",
        "    delayer,\n",
        "    kernel_ridge_cv,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnWyv7614R2q"
      },
      "source": [
        "We can display the ``scikit-learn`` pipeline with an HTML diagram.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdpATEGn4R2q"
      },
      "outputs": [],
      "source": [
        "from sklearn import set_config\n",
        "set_config(display='diagram')  # requires scikit-learn 0.23\n",
        "pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1O1qnxL4R2q"
      },
      "source": [
        "## Fit the model\n",
        "\n",
        "We fit on the training set..\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQaJdk_A4R2r"
      },
      "outputs": [],
      "source": [
        "_ = pipeline.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfUMjWiy4R2r"
      },
      "source": [
        "..and score on the test set. Here the scores are the $R^2$ scores, with\n",
        "values in $]-\\infty, 1]$. A value of $1$ means the predictions\n",
        "are perfect.\n",
        "\n",
        "Note that since ``himalaya`` is implementing multiple-targets\n",
        "models, the ``score`` method differs from ``scikit-learn`` API and returns\n",
        "one score per target/voxel.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0u1y2XfB4R2r"
      },
      "outputs": [],
      "source": [
        "scores = pipeline.score(X_test, Y_test)\n",
        "print(\"(n_voxels,) =\", scores.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Gur0Jq4R2r"
      },
      "source": [
        "If we fit the model on GPU, scores are returned on GPU using an array object\n",
        "specific to the backend we used (such as a ``torch.Tensor``). Thus, we need to\n",
        "move them into ``numpy`` arrays on CPU, to be able to use them for example in\n",
        "a ``matplotlib`` figure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4hESqwi4R2r"
      },
      "outputs": [],
      "source": [
        "scores = backend.to_numpy(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omaLe2Vv4R2r"
      },
      "source": [
        "## Plot the model prediction accuracy\n",
        "\n",
        "To visualize the model prediction accuracy, we can plot it for each voxel on\n",
        "a flattened surface of the brain. To do so, we use a mapper that is specific\n",
        "to the each subject's brain. (Check previous example to see how to use the\n",
        "mapper to Freesurfer average surface.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIRGlWfq4R2r"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from voxelwise_tutorials.viz import plot_flatmap_from_mapper\n",
        "\n",
        "mapper_file = os.path.join(directory, \"mappers\", f\"{subject}_mappers.hdf\")\n",
        "ax = plot_flatmap_from_mapper(scores, mapper_file, vmin=0, vmax=0.4)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AEQJioz4R2r"
      },
      "source": [
        "We can see that the \"wordnet\" features successfully predict part of the\n",
        "measured brain activity, with $R^2$ scores as high as 0.4. Note that\n",
        "these scores are generalization scores, since they are computed on a test set\n",
        "that was not used during model fitting. Since we fitted a model independently\n",
        "in each voxel, we can inspect the generalization performances at the best\n",
        "available spatial resolution: individual voxels.\n",
        "\n",
        "The best-predicted voxels are located in visual semantic areas like EBA, or\n",
        "FFA. This is expected since the wordnet features encode semantic information\n",
        "about the visual stimulus. For more discussions about these results, we refer\n",
        "the reader to the original publication [1]_.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Semantic projections"
      ],
      "metadata": {
        "id": "UWve_yLU9lGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We talked about some limitations of word embeddings and how semantic projections can help find new insights. Let's explore this with actual code!"
      ],
      "metadata": {
        "id": "eSXpSUccVDAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we download the fastText embeddings:"
      ],
      "metadata": {
        "id": "2C7EJAHrtZam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a function to load the fastText embeddings we just downloaded:"
      ],
      "metadata": {
        "id": "uOAqqEKFti8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import io\n",
        "\n",
        "def load_fasttext_aligned_vectors(fname, skip_first_line=True):\n",
        "    '''Return dictionary of word embeddings from file saved in fasttext format.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    fname : str\n",
        "        Name of file containing word embeddings.\n",
        "    skip_first_line : bool\n",
        "        If True, skip first line of file. Should do this if first line of file\n",
        "        contains metadata.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    data : dict\n",
        "        Dictionary of word embeddings.\n",
        "    '''\n",
        "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    if skip_first_line:\n",
        "        n, d = map(int, fin.readline().split())\n",
        "    data = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        data[tokens[0]] = np.array([float(token) for token in tokens[1:]])\n",
        "    return data"
      ],
      "metadata": {
        "id": "MTgg-0hCalV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the fastText embeddings:"
      ],
      "metadata": {
        "id": "sfKHrfJuuWHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load fastText vectors into model object\n",
        "filepath = 'wiki.en.align.vec'\n",
        "model = load_fasttext_aligned_vectors(filepath)"
      ],
      "metadata": {
        "id": "tZb2vYk0avCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a function to compute cosine similarity:"
      ],
      "metadata": {
        "id": "yv0woAlYux0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cos_sim(a, b):\n",
        "   return dot(a, b)/(norm(a)*norm(b))"
      ],
      "metadata": {
        "id": "CP21g4YIpF3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare a few words:"
      ],
      "metadata": {
        "id": "w2wSd5wDu3yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_1 = 'dolphin'\n",
        "word_2 = 'shark'\n",
        "word_3 = 'whale'\n",
        "\n",
        "print(f'Cosine similarity between pairs of words')\n",
        "print(f'({word_1},{word_2}) = {cos_sim(model[word_1], model[word_2])}')\n",
        "print(f'({word_1},{word_3}) = {cos_sim(model[word_1], model[word_3])}')\n",
        "print(f'({word_2},{word_3}) = {cos_sim(model[word_2], model[word_3])}')"
      ],
      "metadata": {
        "id": "iyGQllJheRvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you think of these values?\n",
        "\n",
        "Compare these words with other animals, nouns that aren't animals, words that aren't nouns..."
      ],
      "metadata": {
        "id": "T0VoQAkSvrjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_ = ..."
      ],
      "metadata": {
        "id": "5e_I2cXKwF0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare these words along a specific semantic dimension, namely \"size\".\n",
        "\n",
        "To do so, we will define a vector in the embedding space that represents the size dimension.\n",
        "\n",
        "We first choose words on both ends of the size dimension:"
      ],
      "metadata": {
        "id": "5kxrLK7ywM7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of words\n",
        "list_large = ['large', 'big', 'huge']\n",
        "list_small = ['small', 'little', 'tiny']"
      ],
      "metadata": {
        "id": "BLUpL2vypr94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we compute the difference vector between all pairs (large-small) words and average them:"
      ],
      "metadata": {
        "id": "Tz-36Z60yBcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fastText vectors have size 300\n",
        "size_vector = np.zeros(300)\n",
        "\n",
        "for word_large in list_large:\n",
        "  for word_small in list_small:\n",
        "    size_vector += model[word_large] - model[word_small]\n",
        "size_vector /= len(list_large)*len(list_small)"
      ],
      "metadata": {
        "id": "7VbJG5iRqSgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now project these words on the size dimension!"
      ],
      "metadata": {
        "id": "2WbbOk4NyVo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Semantic projection on size dimension:')\n",
        "print(f'{word_1} = {dot(model[word_1], size_vector)}')\n",
        "print(f'{word_2} = {dot(model[word_2], size_vector)}')\n",
        "print(f'{word_3} = {dot(model[word_3], size_vector)}')"
      ],
      "metadata": {
        "id": "Pn-fBvQsq1XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do these results make sense? Compare other words and see:"
      ],
      "metadata": {
        "id": "o6DUrOGtybWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_ = ...\n",
        "\n",
        "print('Semantic projection on size dimension:')\n",
        "print(f'{word_} = {dot(model[word_], size_vector)}')"
      ],
      "metadata": {
        "id": "5DCMY4cByam2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now compare these words (or other words) on other semantic dimensions!"
      ],
      "metadata": {
        "id": "slSXJqCnscVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_ = ...\n",
        "\n",
        "list_ = ..."
      ],
      "metadata": {
        "id": "njTZpXz_rbjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now use semantic projections with actual brain data! 🧠\n",
        "\n",
        "I trained an encoding model with fastText features on a subject reading english narratives.\n",
        "\n",
        "We thus have model weights that reflect the semantic information of each voxel in the fastText space. Let's download those weights:"
      ],
      "metadata": {
        "id": "lFOQMWu1bC27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1JQzlNB-6z8Z5eEpkaD0q7gIZ-6tThSaX"
      ],
      "metadata": {
        "id": "if7MgXOsbtqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at these weights:"
      ],
      "metadata": {
        "id": "SlXwkXXpcvHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "voxel_weights = np.load('COL_voxel_weights.npz')\n",
        "print(voxel_weights.files)"
      ],
      "metadata": {
        "id": "VX6Q9xM5cOs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voxel_weights['fT_en']"
      ],
      "metadata": {
        "id": "omEqBP9ldEt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voxel_weights['fT_en'].shape"
      ],
      "metadata": {
        "id": "wXPIfjt2dNiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do these dimensions represent?\n",
        "\n",
        "We can now project the weight of each voxel onto our size dimension!"
      ],
      "metadata": {
        "id": "CK-vuDgXdSnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "proj_weights = np.dot(voxel_weights['fT_en'].T, size_vector)\n",
        "print(proj_weights)"
      ],
      "metadata": {
        "id": "Olc6sz07d6oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=11wLAHPLDwkYhDVYXyzJvMtOpJM9f6sfv"
      ],
      "metadata": {
        "id": "62tmsFMkagSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vo3AcnaeahqO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}